{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my_submission.csv', 'sample_submission.csv', 'test.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import kaggle\n",
    "import random\n",
    "import requests\n",
    "import subprocess\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.max_rows\", 20)\n",
    "pd.set_option('display.width', 1000)\n",
    "sns.set_style('white')\n",
    "\n",
    "def get_files(path=\"./data\", extension=\".csv\"):\n",
    "    return [file for file in os.listdir(path) if file[-4:] == extension]\n",
    "\n",
    "def clean(folder_path=\"analysis/\", extension=\".png\"):\n",
    "    for file in os.listdir(folder_path):\n",
    "        print(f\"Removing {file}.\")\n",
    "        os.remove(os.path.join(folder_path, file))\n",
    "\n",
    "print(get_files())\n",
    "clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DataFrame with size: (8693, 13).\n",
      "The type of the columns PassengerId is object.\n",
      "The type of the columns HomePlanet is object.\n",
      "The type of the columns CryoSleep is object.\n",
      "The type of the columns Cabin is object.\n",
      "The type of the columns Destination is object.\n",
      "The type of the columns Age is float64.\n",
      "The type of the columns VIP is object.\n",
      "The type of the columns RoomService is float64.\n",
      "The type of the columns FoodCourt is float64.\n",
      "The type of the columns ShoppingMall is float64.\n",
      "The type of the columns Spa is float64.\n",
      "The type of the columns VRDeck is float64.\n",
      "The type of the columns Transported is bool.\n",
      "Loading DataFrame with size: (4277, 12).\n",
      "The type of the columns PassengerId is object.\n",
      "The type of the columns HomePlanet is object.\n",
      "The type of the columns CryoSleep is object.\n",
      "The type of the columns Cabin is object.\n",
      "The type of the columns Destination is object.\n",
      "The type of the columns Age is float64.\n",
      "The type of the columns VIP is object.\n",
      "The type of the columns RoomService is float64.\n",
      "The type of the columns FoodCourt is float64.\n",
      "The type of the columns ShoppingMall is float64.\n",
      "The type of the columns Spa is float64.\n",
      "The type of the columns VRDeck is float64.\n",
      "\n",
      "               Age     Cabin CryoSleep    Destination  FoodCourt HomePlanet  RoomService  ShoppingMall     Spa Transported    VIP  VRDeck\n",
      "PassengerId                                                                                                                             \n",
      "6403_01      18.0  F/1327/P     False    TRAPPIST-1e        4.0      Earth          0.0           1.0   483.0         NaN  False   205.0\n",
      "0717_01      24.0    B/31/P      True    TRAPPIST-1e        0.0     Europa          0.0           0.0     0.0         NaN  False     0.0\n",
      "3276_01      33.0    A/29/P     False    TRAPPIST-1e      653.0     Europa         20.0           0.0   121.0       False  False   483.0\n",
      "2284_01      18.0   F/463/P     False    TRAPPIST-1e      302.0       Mars        262.0        1127.0  4225.0       False  False     0.0\n",
      "0613_01      53.0    B/26/P      True    TRAPPIST-1e        0.0     Europa          0.0           0.0     0.0         NaN  False     0.0\n",
      "2551_01      32.0   F/523/P     False    TRAPPIST-1e      156.0      Earth        238.0           3.0     2.0       False  False   324.0\n",
      "1024_03      54.0   G/152/P     False    55 Cancri e        5.0        NaN          0.0           0.0   594.0       False  False   209.0\n",
      "4188_01      23.0   F/861/P     False    TRAPPIST-1e       84.0      Earth       1277.0           0.0   240.0        True  False   598.0\n",
      "7510_01      18.0  F/1435/S     False    55 Cancri e        0.0       Mars       1092.0         208.0   150.0       False  False     0.0\n",
      "4214_01      23.0   E/262/P     False    TRAPPIST-1e      343.0     Europa          3.0           0.0  4280.0       False  False     0.0\n",
      "2948_01      55.0   C/112/S     False    TRAPPIST-1e     1971.0     Europa          0.0          77.0   577.0        True  False  1050.0\n",
      "7899_03       1.0  G/1275/S      True    TRAPPIST-1e        0.0      Earth          0.0           0.0     0.0        True  False     0.0\n",
      "0399_01      18.0    G/68/P      True    TRAPPIST-1e        0.0      Earth          0.0           0.0     0.0         NaN  False     0.0\n",
      "8783_02      36.0   D/276/P      True    TRAPPIST-1e        0.0       Mars          0.0           0.0     0.0        True  False     0.0\n",
      "5516_01      21.0  F/1057/S     False    55 Cancri e        0.0      Earth          0.0           9.0     0.0         NaN  False   630.0\n",
      "0110_04      28.0     B/5/P      True    TRAPPIST-1e        0.0     Europa          0.0           0.0     0.0        True  False     0.0\n",
      "5586_01      37.0   G/895/P     False  PSO J318.5-22        0.0      Earth          0.0           6.0   746.0       False  False     1.0\n",
      "8400_01      22.0  F/1612/S     False  PSO J318.5-22      558.0      Earth          0.0           5.0   174.0         NaN  False     0.0\n",
      "8157_06       9.0  G/1313/S     False    TRAPPIST-1e        0.0      Earth          0.0           0.0     0.0        True  False     0.0\n",
      "7172_02      52.0   E/477/S     False    TRAPPIST-1e        0.0       Mars       1350.0           0.0     0.0         NaN  False    19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:30: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_data(file_path=\"data/train.csv\", output_column=[\"Transported\"]):\n",
    "    df = pd.read_csv(\n",
    "        filepath_or_buffer=file_path,\n",
    "        sep=\",\",\n",
    "        usecols=[\n",
    "            \"PassengerId\",\n",
    "            \"HomePlanet\",\n",
    "            \"CryoSleep\",\n",
    "            \"Cabin\",\n",
    "            \"Destination\",\n",
    "            \"Age\",\n",
    "            \"VIP\",\n",
    "            \"RoomService\",\n",
    "            \"FoodCourt\",\n",
    "            \"ShoppingMall\",\n",
    "            \"Spa\",\n",
    "            \"VRDeck\",\n",
    "        ] + output_column,\n",
    "    )\n",
    "    print(f\"Loading DataFrame with size: {df.shape}.\")\n",
    "    if df[\"PassengerId\"].isnull().sum() > 0:\n",
    "        raise ValueError(\"PassengerId null values can not be Greater than 0.\")\n",
    "    for column in df.columns:\n",
    "        print(f\"The type of the columns {column} is {df[column].dtype}.\")\n",
    "    return df.set_index(\"PassengerId\")\n",
    "\n",
    "\n",
    "df = extract_data()\n",
    "df_test = extract_data(file_path=\"data/test.csv\", output_column=[])\n",
    "df = pd.concat([df, df_test])\n",
    "print(\"\\n\", df.sample(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------- NAN Analysis --------\n",
      "For columns: Age, there are 0.02% null or 270 values.\n",
      "For columns: Cabin, there are 0.02% null or 299 values.\n",
      "For columns: CryoSleep, there are 0.02% null or 310 values.\n",
      "For columns: Destination, there are 0.02% null or 274 values.\n",
      "For columns: FoodCourt, there are 0.02% null or 289 values.\n",
      "For columns: HomePlanet, there are 0.02% null or 288 values.\n",
      "For columns: RoomService, there are 0.02% null or 263 values.\n",
      "For columns: ShoppingMall, there are 0.02% null or 306 values.\n",
      "For columns: Spa, there are 0.02% null or 284 values.\n",
      "For columns: Transported, there are 0.33% null or 4277 values.\n",
      "For columns: VIP, there are 0.02% null or 296 values.\n",
      "For columns: VRDeck, there are 0.02% null or 268 values.\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "-------- NAN Analysis --------\n",
      "For columns: Age, there are 0.00% null or 0 values.\n",
      "For columns: CryoSleep, there are 0.00% null or 0 values.\n",
      "For columns: Destination, there are 0.00% null or 0 values.\n",
      "For columns: FoodCourt, there are 0.00% null or 0 values.\n",
      "For columns: HomePlanet, there are 0.00% null or 0 values.\n",
      "For columns: RoomService, there are 0.00% null or 0 values.\n",
      "For columns: ShoppingMall, there are 0.00% null or 0 values.\n",
      "For columns: Spa, there are 0.00% null or 0 values.\n",
      "For columns: Transported, there are 0.33% null or 4277 values.\n",
      "For columns: VIP, there are 0.00% null or 0 values.\n",
      "For columns: VRDeck, there are 0.00% null or 0 values.\n",
      "For columns: total_amount_spent, there are 0.00% null or 0 values.\n",
      "For columns: Age_group, there are 0.00% null or 0 values.\n",
      "For columns: Cabin_deck, there are 0.00% null or 0 values.\n",
      "For columns: Cabin_side, there are 0.00% null or 0 values.\n",
      "--------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def nan_analysis(df):\n",
    "    print(\"\\n-------- NAN Analysis --------\")\n",
    "    for col in df.columns:\n",
    "        nulls = df[col].isnull().sum()\n",
    "        print(f\"For columns: {col}, there are {nulls/df.shape[0]:.2f}% null or {nulls} values.\")\n",
    "    print(\"--------------------------------\\n\")\n",
    "\n",
    "def HomePlanet_analysis(df, analysis_folder=\"analysis\", column=\"HomePlanet\"):\n",
    "    if not os.path.exists(analysis_folder):\n",
    "        os.mkdir(analysis_folder)\n",
    "    fig, axs = plt.subplots(2, 1, sharex=True, figsize=(34, 18))\n",
    "    for index, hue in enumerate([\"VIP\", \"Destination\"]):\n",
    "        sns.catplot(\n",
    "            data=df,\n",
    "            x=column,\n",
    "            y=\"Transported\",\n",
    "            kind=\"bar\",\n",
    "            hue=hue,\n",
    "            ax=axs[index],\n",
    "        )\n",
    "        axs[index].set_title(f\"{column}_hue_{hue}\", fontsize=28)\n",
    "        axs[index].set_xlabel(column, fontsize=28)\n",
    "        axs[index].set_ylabel(\"Transported\", fontsize=28)\n",
    "    fig.savefig(os.path.join(analysis_folder, f\"{column}.png\"))                \n",
    "    for ax in axs:\n",
    "        ax.remove()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def Cabin_analysis(df, analysis_folder=\"analysis\", column=\"Cabin\"):\n",
    "    if not os.path.exists(analysis_folder):\n",
    "        os.mkdir(analysis_folder)\n",
    "    print(\"Creating new column Cabin_deck.\")\n",
    "    df[\"Cabin_deck\"] = df[column].str[0]\n",
    "    print(\"Creating new column Cabin_num.\")\n",
    "    df[\"Cabin_num\"] = df[column].str[2:-2]\n",
    "    print(\"Creating new column Cabin_side.\")\n",
    "    df[\"Cabin_side\"] = df[column].str[-1]\n",
    "    for index, hue in enumerate([\"VIP\", \"Destination\", \"CryoSleep\"]):\n",
    "        print(f\"Creating Cabin_deck/{hue} catplot.\")\n",
    "        fig = plt.figure(figsize=(300, 100))\n",
    "        fig = sns.catplot(\n",
    "            data=df,\n",
    "            x=\"Cabin_deck\",\n",
    "            y=\"Transported\",\n",
    "            kind=\"bar\",\n",
    "            hue=hue,\n",
    "        )\n",
    "        fig.savefig(os.path.join(analysis_folder, f\"{hue}_Cabin_deck.png\"))\n",
    "    for index, hue in enumerate([\"VIP\", \"Destination\", \"CryoSleep\"]):\n",
    "        print(f\"Creating Cabin_side/{hue} catplot.\")\n",
    "        fig = plt.figure(figsize=(300, 100))\n",
    "        fig = sns.catplot(\n",
    "            data=df,\n",
    "            x=\"Cabin_side\",\n",
    "            y=\"Transported\",\n",
    "            kind=\"bar\",\n",
    "            hue=hue,\n",
    "        )\n",
    "        fig.savefig(os.path.join(analysis_folder, f\"{hue}_Cabin_side.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def Age_analysis(df, analysis_folder=\"analysis\", column=\"Age\"):\n",
    "    if not os.path.exists(analysis_folder):\n",
    "        os.mkdir(analysis_folder)\n",
    "    print(\"Creating new columns for Age_group.\")\n",
    "    df[\"Age_group\"] = \"mean\"\n",
    "    df.loc[df[\"Age\"].between(0, 10), \"Age_group\"] = \"0-10\"\n",
    "    df.loc[df[\"Age\"].between(10, 20), \"Age_group\"] = \"10-20\"\n",
    "    df.loc[df[\"Age\"].between(20, 30), \"Age_group\"] = \"20-30\"\n",
    "    df.loc[df[\"Age\"].between(30, 40), \"Age_group\"] = \"30-40\"\n",
    "    df.loc[df[\"Age\"].between(40, 50), \"Age_group\"] = \"40-50\"\n",
    "    df.loc[df[\"Age\"].between(50, 60), \"Age_group\"] = \"50-60\"\n",
    "    df.loc[df[\"Age\"].between(60, 70), \"Age_group\"] = \"60-70\"\n",
    "    df.loc[df[\"Age\"].ge(70), \"Age_group\"] = \">70\"\n",
    "    for index, hue in enumerate([\"VIP\", \"Destination\", \"CryoSleep\", \"HomePlanet\", \"Cabin_deck\", \"Cabin_side\"]):\n",
    "        print(f\"Creating Age_group/{hue} catplot.\")\n",
    "        fig = plt.figure(figsize=(300, 100))\n",
    "        fig = sns.catplot(\n",
    "            data=df,\n",
    "            x=\"Age_group\",\n",
    "            y=\"Transported\",\n",
    "            kind=\"bar\",\n",
    "            hue=hue,\n",
    "        )\n",
    "        fig.savefig(os.path.join(analysis_folder, f\"{hue}_Age_group.png\"))\n",
    "\n",
    "\n",
    "def CryoSleep_analysis(df, analysis_folder=\"analysis\", column=\"CryoSleep\"):\n",
    "    if not os.path.exists(analysis_folder):\n",
    "        os.mkdir(analysis_folder)\n",
    "    for index, hue in enumerate([\"VIP\", \"Destination\", \"HomePlanet\", \"Cabin_deck\", \"Cabin_side\"]):\n",
    "        print(f\"Creating CryoSleep/{hue} catplot.\")\n",
    "        fig = plt.figure(figsize=(300, 100))\n",
    "        fig = sns.catplot(\n",
    "            data=df,\n",
    "            x=\"CryoSleep\",\n",
    "            y=\"Transported\",\n",
    "            kind=\"bar\",\n",
    "            hue=hue,\n",
    "        )\n",
    "        fig.savefig(os.path.join(analysis_folder, f\"{hue}_Age_group.png\"))\n",
    "\n",
    "def fill_nan_values1(df):\n",
    "    new_df = df.copy()\n",
    "    new_df.loc[(new_df[\"HomePlanet\"] == \"Earth\") & (new_df[\"VIP\"].isnull()), \"VIP\"] = \"False\"\n",
    "    new_df.loc[(new_df[\"Cabin_deck\"].isin([\"G\", \"T\"])) & (new_df[\"VIP\"].isnull()), \"VIP\"] = \"False\"\n",
    "    new_df.loc[new_df[\"Age\"].isnull(), \"Age\"] = df[\"Age\"].mean()\n",
    "    new_df.loc[new_df[\"Age_group\"] == \"0-10\", \"VIP\"] = \"False\"\n",
    "    return new_df\n",
    "\n",
    "def fill_nan_values2(df):\n",
    "    new_df = df.copy()\n",
    "    new_df[\"HomePlanet\"].fillna(new_df[\"HomePlanet\"].mode()[0], inplace=True)\n",
    "    new_df[\"CryoSleep\"].fillna(new_df[\"CryoSleep\"].mode()[0], inplace=True)\n",
    "    new_df[\"Destination\"].fillna(new_df[\"Destination\"].mode()[0], inplace=True)\n",
    "    new_df[\"VIP\"].fillna(new_df[\"VIP\"].mode()[0], inplace=True)\n",
    "    new_df[\"Age\"].fillna(new_df[\"Age\"].mean(), inplace=True)\n",
    "    new_df[\"RoomService\"].fillna(new_df[\"RoomService\"].mean(), inplace=True)\n",
    "    new_df[\"FoodCourt\"].fillna(new_df[\"FoodCourt\"].mean(), inplace=True)\n",
    "    new_df[\"ShoppingMall\"].fillna(new_df[\"ShoppingMall\"].mean(), inplace=True)\n",
    "    new_df[\"Spa\"].fillna(new_df[\"Spa\"].mean(), inplace=True)\n",
    "    new_df[\"VRDeck\"].fillna(new_df[\"VRDeck\"].mean(), inplace=True)\n",
    "    return new_df\n",
    "\n",
    "def adding_features(df):\n",
    "    new_df = df.copy()\n",
    "    new_df[\"total_amount_spent\"] = (\n",
    "        new_df[\"RoomService\"] +\n",
    "        new_df[\"FoodCourt\"] +\n",
    "        new_df[\"ShoppingMall\"] +\n",
    "        new_df[\"Spa\"] +\n",
    "        new_df[\"VRDeck\"]\n",
    "    )\n",
    "    new_df.loc[new_df[\"Age\"].between(0, 10), \"Age_group\"] = \"0-10\"\n",
    "    new_df.loc[new_df[\"Age\"].between(10, 20), \"Age_group\"] = \"10-20\"\n",
    "    new_df.loc[new_df[\"Age\"].between(20, 30), \"Age_group\"] = \"20-30\"\n",
    "    new_df.loc[new_df[\"Age\"].between(30, 40), \"Age_group\"] = \"30-40\"\n",
    "    new_df.loc[new_df[\"Age\"].between(40, 50), \"Age_group\"] = \"40-50\"\n",
    "    new_df.loc[new_df[\"Age\"].between(50, 60), \"Age_group\"] = \"50-60\"\n",
    "    new_df.loc[new_df[\"Age\"].between(60, 70), \"Age_group\"] = \"60-70\"\n",
    "    new_df.loc[new_df[\"Age\"].ge(70), \"Age_group\"] = \">70\"\n",
    "    new_df[\"Cabin_deck\"] = new_df[\"Cabin\"].str[0]\n",
    "    new_df[\"Cabin_side\"] = new_df[\"Cabin\"].str[-1]\n",
    "    new_df[\"Cabin_deck\"].fillna(new_df[\"Cabin_deck\"].mode()[0], inplace=True)\n",
    "    new_df[\"Cabin_side\"].fillna(new_df[\"Cabin_side\"].mode()[0], inplace=True)\n",
    "    new_df.drop(columns=[\"Cabin\"], inplace=True)\n",
    "    return new_df\n",
    "\n",
    "nan_analysis(df)\n",
    "new_df = fill_nan_values2(df)\n",
    "ninja = adding_features(new_df)\n",
    "nan_analysis(ninja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enconding column: HomePlanet with LabelEncoder.\n",
      "Enconding column: CryoSleep with LabelEncoder.\n",
      "Enconding column: Destination with LabelEncoder.\n",
      "Enconding column: VIP with LabelEncoder.\n",
      "Enconding column: Age_group with LabelEncoder.\n",
      "Enconding column: Cabin_deck with LabelEncoder.\n",
      "Enconding column: Cabin_side with LabelEncoder.\n",
      "Scaling column: Age with MinMaxScaler.\n",
      "Scaling column: RoomService with MinMaxScaler.\n",
      "Scaling column: FoodCourt with MinMaxScaler.\n",
      "Scaling column: ShoppingMall with MinMaxScaler.\n",
      "Scaling column: Spa with MinMaxScaler.\n",
      "Scaling column: VRDeck with MinMaxScaler.\n",
      "Scaling column: total_amount_spent with MinMaxScaler.\n",
      "\n",
      "                   Age  CryoSleep  Destination  FoodCourt  HomePlanet  RoomService  ShoppingMall       Spa Transported  VIP  VRDeck  total_amount_spent  Age_group  Cabin_deck  Cabin_side\n",
      "PassengerId                                                                                                                                                                              \n",
      "8045_01      0.354430          0            2   0.021165           0     0.028966      0.037843  0.000000         NaN    0     0.0            0.053769          2           5           1\n",
      "6425_01      0.518987          0            2   0.000000           2     0.101557      0.000000  0.000045         NaN    0     0.0            0.040459          4           5           0\n",
      "1345_01      0.316456          0            2   0.000000           0     0.081943      0.000766  0.032176       False    0     0.0            0.053158          2           5           1\n",
      "1331_01      0.303797          1            1   0.000000           0     0.000000      0.000000  0.000000        True    0     0.0            0.000000          2           6           1\n",
      "9145_01      0.430380          1            2   0.000000           0     0.000000      0.000000  0.000000       False    0     0.0            0.000000          3           6           0\n"
     ]
    }
   ],
   "source": [
    "def transform_data(df):\n",
    "    new_df = df.copy()\n",
    "    to_encode = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"Age_group\", \"Cabin_deck\", \"Cabin_side\"]\n",
    "    to_scale = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"total_amount_spent\"]\n",
    "    encoders, scalers = {}, {}\n",
    "    for column in to_encode:\n",
    "        enc = LabelEncoder()\n",
    "        print(f\"Enconding column: {column} with LabelEncoder.\")\n",
    "        if new_df[column].isnull().sum() > 0:\n",
    "            print(f\"Skipping for column: {column}. There are still nan values.\")\n",
    "            continue\n",
    "        new_df[column] = enc.fit_transform(new_df[column])\n",
    "        encoders[column] = enc\n",
    "    for column in to_scale:\n",
    "        sc = MinMaxScaler()\n",
    "        print(f\"Scaling column: {column} with MinMaxScaler.\")\n",
    "        new_df[column] = sc.fit_transform(new_df[column].values.reshape(-1, 1))\n",
    "        scalers[column] = sc\n",
    "    return new_df\n",
    "\n",
    "transformed_df = transform_data(ninja)\n",
    "print(\"\\n\", transformed_df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = transformed_df[~transformed_df[\"Transported\"].isnull()].drop(columns=[\"Transported\"])\n",
    "Y = transformed_df[~transformed_df[\"Transported\"].isnull()][\"Transported\"]\n",
    "enc = LabelEncoder()\n",
    "Y = enc.fit_transform(Y)\n",
    "to_predict = transformed_df[transformed_df[\"Transported\"].isnull()]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\sklearn\\discriminant_analysis.py:463: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(14, 2 - 1) = 1 components.\n",
      "  ChangedBehaviorWarning)\n",
      "D:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\sklearn\\discriminant_analysis.py:469: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
      "  warnings.warn(future_msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=12)\n",
    "X_train = lda.fit_transform(X_train, y_train)\n",
    "X_test = lda.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-8a37ea38e85a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[0mbest_xgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrand_search_xgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;31m# Fit the random search object to the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m \u001b[0mrand_search_rf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;31m# Create a variable for the best model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[0mbest_rf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrand_search_rf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    708\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 710\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1482\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[0;32m   1483\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1484\u001b[1;33m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[1;32mD:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    687\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 689\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    833\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    367\u001b[0m             trees = [self._make_estimator(append=False,\n\u001b[0;32m    368\u001b[0m                                           random_state=random_state)\n\u001b[1;32m--> 369\u001b[1;33m                      for i in range(n_more_estimators)]\n\u001b[0m\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m             \u001b[1;31m# Parallel loop: we prefer the threading backend as the Cython code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    367\u001b[0m             trees = [self._make_estimator(append=False,\n\u001b[0;32m    368\u001b[0m                                           random_state=random_state)\n\u001b[1;32m--> 369\u001b[1;33m                      for i in range(n_more_estimators)]\n\u001b[0m\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m             \u001b[1;31m# Parallel loop: we prefer the threading backend as the Cython code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_base.py\u001b[0m in \u001b[0;36m_make_estimator\u001b[1;34m(self, append, random_state)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0msub\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \"\"\"\n\u001b[1;32m--> 147\u001b[1;33m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_estimator_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m         estimator.set_params(**{p: getattr(self, p)\n\u001b[0;32m    149\u001b[0m                                 for p in self.estimator_params})\n",
      "\u001b[1;32mD:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mnew_object_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mnew_object\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mnew_object_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[0mparams_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;31m# quick sanity check of the parameters of the clone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mget_params\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    187\u001b[0m         \"\"\"\n\u001b[0;32m    188\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_param_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_get_param_names\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;31m# introspect the constructor arguments to find the model parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[1;31m# to represent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         \u001b[0minit_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Consider the constructor parameters excluding 'self'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         parameters = [p for p in init_signature.parameters.values()\n",
      "\u001b[1;32mD:\\Logiciel\\Progra\\Anaconda\\lib\\inspect.py\u001b[0m in \u001b[0;36msignature\u001b[1;34m(obj, follow_wrapped)\u001b[0m\n\u001b[0;32m   3063\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollow_wrapped\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     \u001b[1;34m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3065\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mSignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollow_wrapped\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfollow_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3066\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciel\\Progra\\Anaconda\\lib\\inspect.py\u001b[0m in \u001b[0;36mfrom_callable\u001b[1;34m(cls, obj, follow_wrapped)\u001b[0m\n\u001b[0;32m   2813\u001b[0m         \u001b[1;34m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2814\u001b[0m         return _signature_from_callable(obj, sigcls=cls,\n\u001b[1;32m-> 2815\u001b[1;33m                                         follow_wrapper_chains=follow_wrapped)\n\u001b[0m\u001b[0;32m   2816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2817\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciel\\Progra\\Anaconda\\lib\\inspect.py\u001b[0m in \u001b[0;36m_signature_from_callable\u001b[1;34m(obj, follow_wrapper_chains, skip_bound_arg, sigcls)\u001b[0m\n\u001b[0;32m   2267\u001b[0m         \u001b[1;31m# If it's a pure Python function, or an object that is duck type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2268\u001b[0m         \u001b[1;31m# of a Python function (Cython functions, for instance), then:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2269\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_signature_from_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2271\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_signature_is_builtin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciel\\Progra\\Anaconda\\lib\\inspect.py\u001b[0m in \u001b[0;36m_signature_from_function\u001b[1;34m(cls, func)\u001b[0m\n\u001b[0;32m   2141\u001b[0m     \u001b[1;31m# ... w/ defaults.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2142\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnon_default_count\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2143\u001b[1;33m         \u001b[0mannotation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mannotations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_empty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2144\u001b[0m         parameters.append(Parameter(name, annotation=annotation,\n\u001b[0;32m   2145\u001b[0m                                     \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_POSITIONAL_OR_KEYWORD\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# RandomForest\n",
    "hyper_params_rf = {\n",
    "    \"n_estimators\": [50, 100, 200, 300, 400, 500, 600, 700, 800],\n",
    "    \"max_depth\": [1, 2, 3, 5, 10, 15, 20],\n",
    "}\n",
    "hyper_params_xgb = {\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7],\n",
    "    \"gamma\": [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
    "    \"max_depth \": [0, 1, 2, 3, 4, 5, 6, 10],\n",
    "}\n",
    "hyper_params_cb = {\n",
    "    \"n_estimators\": [50, 100, 150, 200, 300, 400, 500, 600],\n",
    "    \"max_depth\": [2, 4, 8, 12, 15, 20],\n",
    "    \"learning_rate\": [0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5],\n",
    "}\n",
    "hyper_param_lgmb = {\n",
    "    \"num_leaves\": [10, 15, 25, 35, 45, 55, 85, 100],\n",
    "    \"max_depth\": [-1, 2, 4, 8, 10, 12, 15, 20, 25],\n",
    "    \"learning_rate\": [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "}\n",
    "\n",
    "model_lgbm = LGBMClassifier()\n",
    "model_rf = RandomForestClassifier(verbose=0)\n",
    "model_xgb = xgb.XGBClassifier(verbose=False)\n",
    "model_cb = CatBoostClassifier(verbose=False)\n",
    "\n",
    "rand_search_rf = RandomizedSearchCV(\n",
    "    model_rf,\n",
    "    param_distributions=hyper_params_rf,\n",
    "    n_iter=3,\n",
    "    cv=5,\n",
    ")\n",
    "rand_search_xgb = RandomizedSearchCV(\n",
    "    model_xgb,\n",
    "    param_distributions=hyper_params_xgb,\n",
    "    n_iter=3,\n",
    "    cv=5,\n",
    ")\n",
    "rand_search_cat = RandomizedSearchCV(\n",
    "    model_cb,\n",
    "    param_distributions=hyper_params_cb,\n",
    "    n_iter=3,\n",
    "    cv=5,\n",
    ")\n",
    "rand_search_lgmb = RandomizedSearchCV(\n",
    "    model_lgbm,\n",
    "    param_distributions=hyper_param_lgmb,\n",
    "    n_iter=3,\n",
    "    cv=5,\n",
    ")\n",
    "\n",
    "# Fit the random search object to the data\n",
    "rand_search_xgb.fit(X_train, y_train)\n",
    "# Create a variable for the best model\n",
    "best_xgb = rand_search_xgb.best_estimator_\n",
    "# Fit the random search object to the data\n",
    "rand_search_rf.fit(X_train, y_train)\n",
    "# Create a variable for the best model\n",
    "best_rf = rand_search_rf.best_estimator_\n",
    "# Fit the random search object to the data\n",
    "rand_search_cat.fit(X_train, y_train)\n",
    "# Create a variable for the best model\n",
    "best_cb = rand_search_cat.best_estimator_\n",
    "# Fit the random search object to the data\n",
    "rand_search_lgmb.fit(X_train, y_train)\n",
    "# Create a variable for the best model\n",
    "best_cb = rand_search_lgmb.best_estimator_\n",
    "# Print the best hyperparameters\n",
    "print(f\"Best hyperparameters for RF: {rand_search_rf.best_params_}.\")\n",
    "# Print the best hyperparameters\n",
    "print(f\"Best hyperparameters for XGB: {rand_search_xgb.best_params_}.\")\n",
    "# Print the best hyperparameters\n",
    "print(f\"Best hyperparameters for CAT: {rand_search_cat.best_params_}.\")\n",
    "# Print the best hyperparameters\n",
    "print(f\"Best hyperparameters for LGMB: {rand_search_lgmb.best_params_}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LGBMClassifier(num_leaves=100, max_depth=20, learning_rate=0.5)\n",
    "model.fit(X_train, y_train)\n",
    "# Predict values \n",
    "y_predict = model.predict(X_test)\n",
    "# Score\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "print(f\"Accuracy: {accuracy}.\")\n",
    "precision = precision_score(y_test, y_predict)\n",
    "print(f\"Precision: {precision}.\")\n",
    "recall = recall_score(y_test, y_predict)\n",
    "print(f\"Recall: {recall}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=15,\n",
    "    criterion=\"gini\",\n",
    "    verbose=1,\n",
    ")\n",
    "# Train the model with train data\n",
    "model.fit(X_train, y_train)\n",
    "# Predict values \n",
    "y_predict = model.predict(X_test)\n",
    "# Score\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "print(f\"Accuracy: {accuracy}.\")\n",
    "precision = precision_score(y_test, y_predict)\n",
    "print(f\"Precision: {precision}.\")\n",
    "recall = recall_score(y_test, y_predict)\n",
    "print(f\"Recall: {recall}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = xgb.XGBClassifier(\n",
    "    max_depth=4,\n",
    "    learning_rate=0.4,\n",
    "    gamma=0.6,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the random search object to the data\n",
    "model_xgb.fit(X_train, y_train)\n",
    "# Predict values \n",
    "y_predict = model.predict(X_test)\n",
    "# Score\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "print(f\"Accuracy: {accuracy}.\")\n",
    "precision = precision_score(y_test, y_predict)\n",
    "print(f\"Precision: {precision}.\")\n",
    "recall = recall_score(y_test, y_predict)\n",
    "print(f\"Recall: {recall}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cat = CatBoostClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.2,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the random search object to the data\n",
    "model_cat.fit(X_train, y_train)\n",
    "# Predict values \n",
    "y_predict = model_cat.predict(X_test)\n",
    "# Score\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "print(f\"Accuracy: {accuracy}.\")\n",
    "precision = precision_score(y_test, y_predict)\n",
    "print(f\"Precision: {precision}.\")\n",
    "recall = recall_score(y_test, y_predict)\n",
    "print(f\"Recall: {recall}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New_score: 0.8022988505747126\n",
      "New_score: 0.7839080459770115\n",
      "New_score: 0.7425287356321839\n",
      "New_score: 0.7954022988505747\n",
      "New_score: 0.7563218390804598\n",
      "New_score: 0.8\n",
      "New_score: 0.7770114942528735\n",
      "New_score: 0.8091954022988506\n",
      "New_score: 0.7793103448275862\n",
      "New_score: 0.8068965517241379\n",
      "New_score: 0.7977011494252874\n",
      "New_score: 0.8022988505747126\n",
      "New_score: 0.8137931034482758\n",
      "New_score: 0.8179723502304147\n",
      "New_score: 0.783410138248848\n",
      "New_score: 0.8456221198156681\n",
      "New_score: 0.815668202764977\n",
      "New_score: 0.8294930875576036\n",
      "New_score: 0.8064516129032258\n",
      "New_score: 0.7672811059907834\n"
     ]
    }
   ],
   "source": [
    "# KFold & Ensemble learning\n",
    "kfold = KFold(n_splits=20)\n",
    "cat_model = CatBoostClassifier(n_estimators=400, max_depth=8, learning_rate=0.4, verbose=False)\n",
    "xgb_model = xgb.XGBClassifier(max_depth=5, learning_rate=0.02, gamma=0.1, verbose=False)\n",
    "rf_model = RandomForestClassifier(n_estimators=700, max_depth=2, verbose=0)\n",
    "model_lgbm = LGBMClassifier(num_leaves=15, max_depth=12, learning_rate=0.05)\n",
    "\n",
    "estimators = [(\"catboost\", cat_model), (\"XGBoost\", xgb_model), (\"RandomForest\", rf_model), (\"LGBM\", model_lgbm)]\n",
    "best_score, best_model = 0, None\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    ensemble = VotingClassifier(estimators)\n",
    "    x_train, x_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    ensemble.fit(x_train,y_train)\n",
    "    new_score = ensemble.score(x_test, y_test)\n",
    "    print(f\"New_score: {new_score}\")\n",
    "    if new_score > best_score:\n",
    "        best_score = new_score\n",
    "        best_model = ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "D:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "to_predict[\"Transported\"] = best_model.predict(to_predict.drop(columns=[\"Transported\"]))\n",
    "to_predict[\"Transported\"] = enc.inverse_transform(to_predict[\"Transported\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry: (4277, 1).\n",
      "Output: (4277, 16).\n",
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.15 / client 1.5.13)\n",
      "Successfully submitted to Spaceship Titanic\n"
     ]
    }
   ],
   "source": [
    "def generate_submission_file_based_on_prediction(result, source_file_path=\"data/sample_submission.csv\"):\n",
    "    df = pd.read_csv(\n",
    "        filepath_or_buffer=source_file_path,\n",
    "        sep=\",\",\n",
    "    ).set_index(\"PassengerId\")\n",
    "    print(f\"Entry: {df.shape}.\")\n",
    "    merge_df = pd.merge(\n",
    "        left=df,\n",
    "        right=result,\n",
    "        how=\"left\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "    print(f\"Output: {merge_df.shape}.\")\n",
    "    if df.shape[0] != merge_df.shape[0]:\n",
    "        raise ValueError(f\"Should be same size.\")\n",
    "    merge_df = merge_df[[\"Transported_x\", \"Transported_y\"]]\n",
    "    merge_df = merge_df.reset_index().rename(columns={\"Transported_y\": \"Transported\"}).drop(columns=[\"Transported_x\"])\n",
    "    merge_df.to_csv(\"data/my_submission.csv\", sep=\",\", index=False)\n",
    "    \n",
    "def generate_random_submission(source_file_path=\"data/sample_submission.csv\"):\n",
    "    df = pd.read_csv(\n",
    "        filepath_or_buffer=source_file_path,\n",
    "        sep=\",\",\n",
    "    )\n",
    "    df[\"Transported\"] = df[\"Transported\"].apply(lambda x: random.choice([\"False\", \"True\"]))\n",
    "    print(f\"File data/my_submission.csv successfully generated.\\n\")\n",
    "    df.to_csv(\"data/my_submission.csv\", sep=\",\", index=False)\n",
    "    \n",
    "def submit_submission(competition_id=\"2357\", submission_file=\"data/my_submission.csv\"):\n",
    "    with open(\"kaggle.json\") as credential:\n",
    "        json_credential = json.loads(credential.read())\n",
    "        os.environ[\"KAGGLE_USERNAME\"] = json_credential[\"username\"]\n",
    "        os.environ[\"KAGGLE_KEY\"] = json_credential[\"key\"]\n",
    "    result = subprocess.check_output(\n",
    "        [\n",
    "            \"kaggle\",\n",
    "            \"competitions\",\n",
    "            \"submit\",\n",
    "            \"spaceship-titanic\",\n",
    "            \"-f\",\n",
    "            submission_file,\n",
    "            \"-m\",\n",
    "            f\"{dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: New submission\",\n",
    "        ]\n",
    "    ).decode(\"utf-8\")\n",
    "    print(result)\n",
    "\n",
    "def get_latest_score(competition_id=\"2357\", team_id=\"10059555\"):\n",
    "    with open(\"kaggle.json\") as credential:\n",
    "        json_credential = json.loads(credential.read())\n",
    "        os.environ[\"KAGGLE_USERNAME\"] = json_credential[\"username\"]\n",
    "        os.environ[\"KAGGLE_KEY\"] = json_credential[\"key\"]\n",
    "        os.environ[\"KAGGLE_TEAM_ID\"] = team_id\n",
    "    result = subprocess.check_output([\"kaggle\", \"competitions\", \"submissions\", \"spaceship-titanic\"]).decode(\"utf-8\")\n",
    "    print(result)\n",
    "\n",
    "generate_submission_file_based_on_prediction(to_predict)\n",
    "submit_submission()\n",
    "# get_latest_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.15 / client 1.5.13)\r\n",
      "fileName               date                 description                          status    publicScore  privateScore  \r\n",
      "---------------------  -------------------  -----------------------------------  --------  -----------  ------------  \r\n",
      "my_submission.csv      2023-08-23 18:44:21  2023-08-23 19:44:18: New submission  complete  0.79798                    \r\n",
      "my_submission.csv      2023-03-24 11:21:24  2023-03-24 11:21:18: New submission  complete  0.79822                    \r\n",
      "my_submission.csv      2023-03-24 10:40:46  2023-03-24 10:40:42: New submission  complete  0.79120                    \r\n",
      "my_submission.csv      2023-03-24 10:36:54  2023-03-24 10:36:49: New submission  complete  0.79869                    \r\n",
      "my_submission.csv      2023-03-24 10:32:32  2023-03-24 10:32:18: New submission  complete  0.79448                    \r\n",
      "my_submission.csv      2023-03-17 15:44:11  2023-03-17 15:44:09: New submission  complete  0.79354                    \r\n",
      "my_submission.csv      2023-03-17 15:38:54  2023-03-17 15:38:49: New submission  complete  0.79845                    \r\n",
      "my_submission.csv      2023-03-17 11:44:11  2023-03-17 11:44:09: New submission  complete  0.79869                    \r\n",
      "my_submission.csv      2023-03-17 11:19:19  2023-03-17 11:19:12: New submission  complete  0.79845                    \r\n",
      "my_submission.csv      2023-03-17 09:53:09  2023-03-17 09:53:05: New submission  complete  0.79050                    \r\n",
      "my_submission.csv      2023-03-16 17:30:10  2023-03-16 17:30:06: New submission  complete  0.79354                    \r\n",
      "my_submission.csv      2023-03-16 17:02:46  2023-03-16 17:02:42: New submission  complete  0.79682                    \r\n",
      "my_submission.csv      2023-03-16 15:18:08  2023-03-16 15:18:05: New submission  complete  0.49964                    \r\n",
      "sample_submission.csv  2023-03-16 10:14:41  2023-03-16 10:14:38: New submission  complete  0.49310                    \r\n",
      "sample_submission.csv  2023-03-16 09:47:43  My first submission message          complete  0.49310                    \r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_latest_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
