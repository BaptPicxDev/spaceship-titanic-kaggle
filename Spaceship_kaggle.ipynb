{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-17-16c26201ce7f>, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-17-16c26201ce7f>\"\u001b[1;36m, line \u001b[1;32m27\u001b[0m\n\u001b[1;33m    return [file for file in os.listdir(path) if file[-4:] == extension]\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import kaggle\n",
    "import random\n",
    "import requests\n",
    "import subprocess\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.max_rows\", 20)\n",
    "pd.set_option('display.width', 1000)\n",
    "sns.set_style('white')\n",
    "    return [file for file in os.listdir(path) if file[-4:] == extension]\n",
    "\n",
    "def clean(folder_path=\"analysis/\", extension=\".png\"):\n",
    "    for file in os.listdir(folder_path):\n",
    "        print(f\"Removing {file}.\")\n",
    "        os.remove(os.path.join(folder_path, file))\n",
    "\n",
    "clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DataFrame with size: (8693, 13).\n",
      "The type of the columns PassengerId is object.\n",
      "The type of the columns HomePlanet is object.\n",
      "The type of the columns CryoSleep is object.\n",
      "The type of the columns Cabin is object.\n",
      "The type of the columns Destination is object.\n",
      "The type of the columns Age is float64.\n",
      "The type of the columns VIP is object.\n",
      "The type of the columns RoomService is float64.\n",
      "The type of the columns FoodCourt is float64.\n",
      "The type of the columns ShoppingMall is float64.\n",
      "The type of the columns Spa is float64.\n",
      "The type of the columns VRDeck is float64.\n",
      "The type of the columns Transported is bool.\n",
      "Loading DataFrame with size: (4277, 12).\n",
      "The type of the columns PassengerId is object.\n",
      "The type of the columns HomePlanet is object.\n",
      "The type of the columns CryoSleep is object.\n",
      "The type of the columns Cabin is object.\n",
      "The type of the columns Destination is object.\n",
      "The type of the columns Age is float64.\n",
      "The type of the columns VIP is object.\n",
      "The type of the columns RoomService is float64.\n",
      "The type of the columns FoodCourt is float64.\n",
      "The type of the columns ShoppingMall is float64.\n",
      "The type of the columns Spa is float64.\n",
      "The type of the columns VRDeck is float64.\n",
      "\n",
      "               Age     Cabin CryoSleep    Destination  FoodCourt HomePlanet  RoomService  ShoppingMall     Spa Transported    VIP   VRDeck\n",
      "PassengerId                                                                                                                              \n",
      "5322_01      22.0  F/1090/P      True    TRAPPIST-1e        0.0       Mars          0.0           0.0     NaN        True  False      NaN\n",
      "6257_03       0.0  G/1010/P      True    TRAPPIST-1e        0.0      Earth          0.0           0.0     0.0         NaN  False      0.0\n",
      "9243_01      35.0   E/596/P     False    TRAPPIST-1e     4813.0     Europa          0.0           0.0   142.0         NaN  False    523.0\n",
      "6380_03      16.0  F/1322/P     False    55 Cancri e      511.0      Earth          2.0           0.0     0.0       False  False   1578.0\n",
      "1203_01      50.0    B/51/S     False    55 Cancri e     5473.0     Europa          0.0           0.0   703.0       False   True   6381.0\n",
      "1234_01      23.0   F/236/S     False  PSO J318.5-22        0.0      Earth          0.0        1250.0     0.0        True  False      0.0\n",
      "8846_01      34.0   C/328/S     False    TRAPPIST-1e       65.0     Europa         28.0        3429.0  6555.0         NaN    NaN  12113.0\n",
      "8705_01      20.0  F/1790/P       NaN    55 Cancri e        0.0       Mars          0.0           0.0     0.0         NaN  False      0.0\n",
      "4337_01      49.0   F/813/S     False    TRAPPIST-1e        0.0       Mars         22.0        1129.0     0.0        True  False     77.0\n",
      "8419_01      19.0  F/1618/S     False    TRAPPIST-1e      168.0      Earth          0.0         481.0     0.0        True  False      0.0\n",
      "4878_01      36.0   F/929/S     False    TRAPPIST-1e        0.0       Mars        108.0         941.0   861.0       False  False      0.0\n",
      "0714_03      12.0   G/109/P     False            NaN        0.0      Earth          0.0           NaN     0.0        True  False      0.0\n",
      "8534_03       NaN  G/1371/S      True    TRAPPIST-1e        0.0      Earth          0.0           0.0     0.0         NaN  False      0.0\n",
      "1862_02      35.0   G/304/P     False    55 Cancri e        0.0        NaN        557.0           0.0   393.0         NaN  False      0.0\n",
      "2560_02      21.0   G/412/P     False    TRAPPIST-1e        0.0      Earth        882.0           0.0     NaN       False  False      0.0\n",
      "2703_01      18.0   G/429/S     False    TRAPPIST-1e      617.0      Earth          0.0           0.0     0.0       False  False     10.0\n",
      "7452_03      36.0   B/281/S      True    55 Cancri e        0.0     Europa          0.0           0.0     NaN        True  False      0.0\n",
      "8050_02       1.0       NaN      True    TRAPPIST-1e        0.0      Earth          0.0           0.0     0.0        True  False      0.0\n",
      "4295_01      23.0   D/147/P     False    55 Cancri e        0.0       Mars         49.0        2716.0     0.0        True  False      0.0\n",
      "4768_01      56.0   D/158/P     False    TRAPPIST-1e     2345.0     Europa          0.0          39.0   338.0         NaN  False    452.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:30: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_data(file_path=\"data/train.csv\", output_column=[\"Transported\"]):\n",
    "    df = pd.read_csv(\n",
    "        filepath_or_buffer=file_path,\n",
    "        sep=\",\",\n",
    "        usecols=[\n",
    "            \"PassengerId\",\n",
    "            \"HomePlanet\",\n",
    "            \"CryoSleep\",\n",
    "            \"Cabin\",\n",
    "            \"Destination\",\n",
    "            \"Age\",\n",
    "            \"VIP\",\n",
    "            \"RoomService\",\n",
    "            \"FoodCourt\",\n",
    "            \"ShoppingMall\",\n",
    "            \"Spa\",\n",
    "            \"VRDeck\",\n",
    "        ] + output_column,\n",
    "    )\n",
    "    print(f\"Loading DataFrame with size: {df.shape}.\")\n",
    "    if df[\"PassengerId\"].isnull().sum() > 0:\n",
    "        raise ValueError(\"PassengerId null values can not be Greater than 0.\")\n",
    "    for column in df.columns:\n",
    "        print(f\"The type of the columns {column} is {df[column].dtype}.\")\n",
    "    return df.set_index(\"PassengerId\")\n",
    "\n",
    "\n",
    "df = extract_data()\n",
    "df_test = extract_data(file_path=\"data/test.csv\", output_column=[])\n",
    "df = pd.concat([df, df_test])\n",
    "print(\"\\n\", df.sample(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------- NAN Analysis --------\n",
      "For columns: Age, there are 0.02% null or 270 values.\n",
      "For columns: Cabin, there are 0.02% null or 299 values.\n",
      "For columns: CryoSleep, there are 0.02% null or 310 values.\n",
      "For columns: Destination, there are 0.02% null or 274 values.\n",
      "For columns: FoodCourt, there are 0.02% null or 289 values.\n",
      "For columns: HomePlanet, there are 0.02% null or 288 values.\n",
      "For columns: RoomService, there are 0.02% null or 263 values.\n",
      "For columns: ShoppingMall, there are 0.02% null or 306 values.\n",
      "For columns: Spa, there are 0.02% null or 284 values.\n",
      "For columns: Transported, there are 0.33% null or 4277 values.\n",
      "For columns: VIP, there are 0.02% null or 296 values.\n",
      "For columns: VRDeck, there are 0.02% null or 268 values.\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "-------- NAN Analysis --------\n",
      "For columns: Age, there are 0.00% null or 0 values.\n",
      "For columns: CryoSleep, there are 0.00% null or 0 values.\n",
      "For columns: Destination, there are 0.00% null or 0 values.\n",
      "For columns: FoodCourt, there are 0.00% null or 0 values.\n",
      "For columns: HomePlanet, there are 0.00% null or 0 values.\n",
      "For columns: RoomService, there are 0.00% null or 0 values.\n",
      "For columns: ShoppingMall, there are 0.00% null or 0 values.\n",
      "For columns: Spa, there are 0.00% null or 0 values.\n",
      "For columns: Transported, there are 0.33% null or 4277 values.\n",
      "For columns: VIP, there are 0.00% null or 0 values.\n",
      "For columns: VRDeck, there are 0.00% null or 0 values.\n",
      "For columns: total_amount_spent, there are 0.00% null or 0 values.\n",
      "For columns: Age_group, there are 0.00% null or 0 values.\n",
      "For columns: Cabin_deck, there are 0.00% null or 0 values.\n",
      "For columns: Cabin_side, there are 0.00% null or 0 values.\n",
      "--------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def nan_analysis(df):\n",
    "    print(\"\\n-------- NAN Analysis --------\")\n",
    "    for col in df.columns:\n",
    "        nulls = df[col].isnull().sum()\n",
    "        print(f\"For columns: {col}, there are {nulls/df.shape[0]:.2f}% null or {nulls} values.\")\n",
    "    print(\"--------------------------------\\n\")\n",
    "\n",
    "def HomePlanet_analysis(df, analysis_folder=\"analysis\", column=\"HomePlanet\"):\n",
    "    if not os.path.exists(analysis_folder):\n",
    "        os.mkdir(analysis_folder)\n",
    "    fig, axs = plt.subplots(2, 1, sharex=True, figsize=(34, 18))\n",
    "    for index, hue in enumerate([\"VIP\", \"Destination\"]):\n",
    "        sns.catplot(\n",
    "            data=df,\n",
    "            x=column,\n",
    "            y=\"Transported\",\n",
    "            kind=\"bar\",\n",
    "            hue=hue,\n",
    "            ax=axs[index],\n",
    "        )\n",
    "        axs[index].set_title(f\"{column}_hue_{hue}\", fontsize=28)\n",
    "        axs[index].set_xlabel(column, fontsize=28)\n",
    "        axs[index].set_ylabel(\"Transported\", fontsize=28)\n",
    "    fig.savefig(os.path.join(analysis_folder, f\"{column}.png\"))                \n",
    "    for ax in axs:\n",
    "        ax.remove()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def Cabin_analysis(df, analysis_folder=\"analysis\", column=\"Cabin\"):\n",
    "    if not os.path.exists(analysis_folder):\n",
    "        os.mkdir(analysis_folder)\n",
    "    print(\"Creating new column Cabin_deck.\")\n",
    "    df[\"Cabin_deck\"] = df[column].str[0]\n",
    "    print(\"Creating new column Cabin_num.\")\n",
    "    df[\"Cabin_num\"] = df[column].str[2:-2]\n",
    "    print(\"Creating new column Cabin_side.\")\n",
    "    df[\"Cabin_side\"] = df[column].str[-1]\n",
    "    for index, hue in enumerate([\"VIP\", \"Destination\", \"CryoSleep\"]):\n",
    "        print(f\"Creating Cabin_deck/{hue} catplot.\")\n",
    "        fig = plt.figure(figsize=(300, 100))\n",
    "        fig = sns.catplot(\n",
    "            data=df,\n",
    "            x=\"Cabin_deck\",\n",
    "            y=\"Transported\",\n",
    "            kind=\"bar\",\n",
    "            hue=hue,\n",
    "        )\n",
    "        fig.savefig(os.path.join(analysis_folder, f\"{hue}_Cabin_deck.png\"))\n",
    "    for index, hue in enumerate([\"VIP\", \"Destination\", \"CryoSleep\"]):\n",
    "        print(f\"Creating Cabin_side/{hue} catplot.\")\n",
    "        fig = plt.figure(figsize=(300, 100))\n",
    "        fig = sns.catplot(\n",
    "            data=df,\n",
    "            x=\"Cabin_side\",\n",
    "            y=\"Transported\",\n",
    "            kind=\"bar\",\n",
    "            hue=hue,\n",
    "        )\n",
    "        fig.savefig(os.path.join(analysis_folder, f\"{hue}_Cabin_side.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def Age_analysis(df, analysis_folder=\"analysis\", column=\"Age\"):\n",
    "    if not os.path.exists(analysis_folder):\n",
    "        os.mkdir(analysis_folder)\n",
    "    print(\"Creating new columns for Age_group.\")\n",
    "    df[\"Age_group\"] = \"mean\"\n",
    "    df.loc[df[\"Age\"].between(0, 10), \"Age_group\"] = \"0-10\"\n",
    "    df.loc[df[\"Age\"].between(10, 20), \"Age_group\"] = \"10-20\"\n",
    "    df.loc[df[\"Age\"].between(20, 30), \"Age_group\"] = \"20-30\"\n",
    "    df.loc[df[\"Age\"].between(30, 40), \"Age_group\"] = \"30-40\"\n",
    "    df.loc[df[\"Age\"].between(40, 50), \"Age_group\"] = \"40-50\"\n",
    "    df.loc[df[\"Age\"].between(50, 60), \"Age_group\"] = \"50-60\"\n",
    "    df.loc[df[\"Age\"].between(60, 70), \"Age_group\"] = \"60-70\"\n",
    "    df.loc[df[\"Age\"].ge(70), \"Age_group\"] = \">70\"\n",
    "    for index, hue in enumerate([\"VIP\", \"Destination\", \"CryoSleep\", \"HomePlanet\", \"Cabin_deck\", \"Cabin_side\"]):\n",
    "        print(f\"Creating Age_group/{hue} catplot.\")\n",
    "        fig = plt.figure(figsize=(300, 100))\n",
    "        fig = sns.catplot(\n",
    "            data=df,\n",
    "            x=\"Age_group\",\n",
    "            y=\"Transported\",\n",
    "            kind=\"bar\",\n",
    "            hue=hue,\n",
    "        )\n",
    "        fig.savefig(os.path.join(analysis_folder, f\"{hue}_Age_group.png\"))\n",
    "\n",
    "\n",
    "def CryoSleep_analysis(df, analysis_folder=\"analysis\", column=\"CryoSleep\"):\n",
    "    if not os.path.exists(analysis_folder):\n",
    "        os.mkdir(analysis_folder)\n",
    "    for index, hue in enumerate([\"VIP\", \"Destination\", \"HomePlanet\", \"Cabin_deck\", \"Cabin_side\"]):\n",
    "        print(f\"Creating CryoSleep/{hue} catplot.\")\n",
    "        fig = plt.figure(figsize=(300, 100))\n",
    "        fig = sns.catplot(\n",
    "            data=df,\n",
    "            x=\"CryoSleep\",\n",
    "            y=\"Transported\",\n",
    "            kind=\"bar\",\n",
    "            hue=hue,\n",
    "        )\n",
    "        fig.savefig(os.path.join(analysis_folder, f\"{hue}_Age_group.png\"))\n",
    "\n",
    "def fill_nan_values1(df):\n",
    "    new_df = df.copy()\n",
    "    new_df.loc[(new_df[\"HomePlanet\"] == \"Earth\") & (new_df[\"VIP\"].isnull()), \"VIP\"] = \"False\"\n",
    "    new_df.loc[(new_df[\"Cabin_deck\"].isin([\"G\", \"T\"])) & (new_df[\"VIP\"].isnull()), \"VIP\"] = \"False\"\n",
    "    new_df.loc[new_df[\"Age\"].isnull(), \"Age\"] = df[\"Age\"].mean()\n",
    "    new_df.loc[new_df[\"Age_group\"] == \"0-10\", \"VIP\"] = \"False\"\n",
    "    return new_df\n",
    "\n",
    "def fill_nan_values2(df):\n",
    "    new_df = df.copy()\n",
    "    new_df[\"HomePlanet\"].fillna(new_df[\"HomePlanet\"].mode()[0], inplace=True)\n",
    "    new_df[\"CryoSleep\"].fillna(new_df[\"CryoSleep\"].mode()[0], inplace=True)\n",
    "    new_df[\"Destination\"].fillna(new_df[\"Destination\"].mode()[0], inplace=True)\n",
    "    new_df[\"VIP\"].fillna(new_df[\"VIP\"].mode()[0], inplace=True)\n",
    "    new_df[\"Age\"].fillna(new_df[\"Age\"].mean(), inplace=True)\n",
    "    new_df[\"RoomService\"].fillna(new_df[\"RoomService\"].mean(), inplace=True)\n",
    "    new_df[\"FoodCourt\"].fillna(new_df[\"FoodCourt\"].mean(), inplace=True)\n",
    "    new_df[\"ShoppingMall\"].fillna(new_df[\"ShoppingMall\"].mean(), inplace=True)\n",
    "    new_df[\"Spa\"].fillna(new_df[\"Spa\"].mean(), inplace=True)\n",
    "    new_df[\"VRDeck\"].fillna(new_df[\"VRDeck\"].mean(), inplace=True)\n",
    "    return new_df\n",
    "\n",
    "def adding_features(df):\n",
    "    new_df = df.copy()\n",
    "    new_df[\"total_amount_spent\"] = (\n",
    "        new_df[\"RoomService\"] +\n",
    "        new_df[\"FoodCourt\"] +\n",
    "        new_df[\"ShoppingMall\"] +\n",
    "        new_df[\"Spa\"] +\n",
    "        new_df[\"VRDeck\"]\n",
    "    )\n",
    "    new_df.loc[new_df[\"Age\"].between(0, 10), \"Age_group\"] = \"0-10\"\n",
    "    new_df.loc[new_df[\"Age\"].between(10, 20), \"Age_group\"] = \"10-20\"\n",
    "    new_df.loc[new_df[\"Age\"].between(20, 30), \"Age_group\"] = \"20-30\"\n",
    "    new_df.loc[new_df[\"Age\"].between(30, 40), \"Age_group\"] = \"30-40\"\n",
    "    new_df.loc[new_df[\"Age\"].between(40, 50), \"Age_group\"] = \"40-50\"\n",
    "    new_df.loc[new_df[\"Age\"].between(50, 60), \"Age_group\"] = \"50-60\"\n",
    "    new_df.loc[new_df[\"Age\"].between(60, 70), \"Age_group\"] = \"60-70\"\n",
    "    new_df.loc[new_df[\"Age\"].ge(70), \"Age_group\"] = \">70\"\n",
    "    new_df[\"Cabin_deck\"] = new_df[\"Cabin\"].str[0]\n",
    "    new_df[\"Cabin_side\"] = new_df[\"Cabin\"].str[-1]\n",
    "    new_df[\"Cabin_deck\"].fillna(new_df[\"Cabin_deck\"].mode()[0], inplace=True)\n",
    "    new_df[\"Cabin_side\"].fillna(new_df[\"Cabin_side\"].mode()[0], inplace=True)\n",
    "    new_df.drop(columns=[\"Cabin\"], inplace=True)\n",
    "    return new_df\n",
    "\n",
    "nan_analysis(df)\n",
    "new_df = fill_nan_values2(df)\n",
    "ninja = adding_features(new_df)\n",
    "nan_analysis(ninja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enconding column: HomePlanet with LabelEncoder.\n",
      "Enconding column: CryoSleep with LabelEncoder.\n",
      "Enconding column: Destination with LabelEncoder.\n",
      "Enconding column: VIP with LabelEncoder.\n",
      "Enconding column: Age_group with LabelEncoder.\n",
      "Enconding column: Cabin_deck with LabelEncoder.\n",
      "Enconding column: Cabin_side with LabelEncoder.\n",
      "Scaling column: Age with MinMaxScaler.\n",
      "Scaling column: RoomService with MinMaxScaler.\n",
      "Scaling column: FoodCourt with MinMaxScaler.\n",
      "Scaling column: ShoppingMall with MinMaxScaler.\n",
      "Scaling column: Spa with MinMaxScaler.\n",
      "Scaling column: VRDeck with MinMaxScaler.\n",
      "Scaling column: total_amount_spent with MinMaxScaler.\n",
      "\n",
      "                   Age  CryoSleep  Destination  FoodCourt  HomePlanet  RoomService  ShoppingMall       Spa Transported  VIP    VRDeck  total_amount_spent  Age_group  Cabin_deck  Cabin_side\n",
      "PassengerId                                                                                                                                                                                \n",
      "6816_03      0.405063          1            1   0.000000           0     0.000000       0.00000  0.000000        True    0  0.000000            0.000000          3           6           0\n",
      "2754_01      0.164557          0            2   0.000034           0     0.063586       0.00000  0.003302       False    0  0.000000            0.027399          1           4           0\n",
      "1187_03      0.139241          0            2   0.000000           0     0.000000       0.00000  0.000000       False    0  0.000000            0.000000          1           6           1\n",
      "2505_02      0.240506          0            2   0.000168           0     0.000000       0.00017  0.024143       False    0  0.009862            0.021897          1           4           0\n",
      "2341_01      0.189873          0            2   0.026331           0     0.000000       0.00000  0.000000        True    0  0.000000            0.021813          1           5           1\n"
     ]
    }
   ],
   "source": [
    "def transform_data(df):\n",
    "    new_df = df.copy()\n",
    "    to_encode = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"Age_group\", \"Cabin_deck\", \"Cabin_side\"]\n",
    "    to_scale = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"total_amount_spent\"]\n",
    "    encoders, scalers = {}, {}\n",
    "    for column in to_encode:\n",
    "        enc = LabelEncoder()\n",
    "        print(f\"Enconding column: {column} with LabelEncoder.\")\n",
    "        if new_df[column].isnull().sum() > 0:\n",
    "            print(f\"Skipping for column: {column}. There are still nan values.\")\n",
    "            continue\n",
    "        new_df[column] = enc.fit_transform(new_df[column])\n",
    "        encoders[column] = enc\n",
    "    for column in to_scale:\n",
    "        sc = MinMaxScaler()\n",
    "        print(f\"Scaling column: {column} with MinMaxScaler.\")\n",
    "        new_df[column] = sc.fit_transform(new_df[column].values.reshape(-1, 1))\n",
    "        scalers[column] = sc\n",
    "    return new_df\n",
    "\n",
    "transformed_df = transform_data(ninja)\n",
    "print(\"\\n\", transformed_df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = transformed_df[~transformed_df[\"Transported\"].isnull()].drop(columns=[\"Transported\"])\n",
    "Y = transformed_df[~transformed_df[\"Transported\"].isnull()][\"Transported\"]\n",
    "enc = LabelEncoder()\n",
    "Y = enc.fit_transform(Y)\n",
    "to_predict = transformed_df[transformed_df[\"Transported\"].isnull()]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\sklearn\\discriminant_analysis.py:463: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(14, 2 - 1) = 1 components.\n",
      "  ChangedBehaviorWarning)\n",
      "D:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\sklearn\\discriminant_analysis.py:469: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
      "  warnings.warn(future_msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=12)\n",
    "X_train = lda.fit_transform(X_train, y_train)\n",
    "X_test = lda.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RandomForest\n",
    "hyper_params_rf = {\n",
    "    \"n_estimators\": [50, 100, 200, 300, 400, 500, 600, 700, 800],\n",
    "    \"max_depth\": [1, 2, 3, 5, 10, 15, 20],\n",
    "}\n",
    "hyper_params_xgb = {\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7],\n",
    "    \"gamma\": [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
    "    \"max_depth \": [0, 1, 2, 3, 4, 5, 6, 10],\n",
    "}\n",
    "hyper_params_cb = {\n",
    "    \"n_estimators\": [50, 100, 150, 200, 300, 400, 500, 600],\n",
    "    \"max_depth\": [2, 4, 8, 12, 15, 20],\n",
    "    \"learning_rate\": [0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5],\n",
    "}\n",
    "hyper_param_lgmb = {\n",
    "    \"num_leaves\": [10, 15, 25, 35, 45, 55, 85, 100],\n",
    "    \"max_depth\": [-1, 2, 4, 8, 10, 12, 15, 20, 25],\n",
    "    \"learning_rate\": [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "}\n",
    "\n",
    "model_lgbm = LGBMClassifier()\n",
    "model_rf = RandomForestClassifier(verbose=0)\n",
    "model_xgb = xgb.XGBClassifier(verbose=False)\n",
    "model_cb = CatBoostClassifier(verbose=False)\n",
    "\n",
    "rand_search_rf = RandomizedSearchCV(\n",
    "    model_rf,\n",
    "    param_distributions=hyper_params_rf,\n",
    "    n_iter=3,\n",
    "    cv=5,\n",
    ")\n",
    "rand_search_xgb = RandomizedSearchCV(\n",
    "    model_xgb,\n",
    "    param_distributions=hyper_params_xgb,\n",
    "    n_iter=3,\n",
    "    cv=5,\n",
    ")\n",
    "rand_search_cat = RandomizedSearchCV(\n",
    "    model_cb,\n",
    "    param_distributions=hyper_params_cb,\n",
    "    n_iter=3,\n",
    "    cv=5,\n",
    ")\n",
    "rand_search_lgmb = RandomizedSearchCV(\n",
    "    model_lgbm,\n",
    "    param_distributions=hyper_param_lgmb,\n",
    "    n_iter=3,\n",
    "    cv=5,\n",
    ")\n",
    "# Fit the random search object to the data\n",
    "rand_search_xgb.fit(X_train, y_train)\n",
    "# Create a variable for the best model\n",
    "best_xgb = rand_search_xgb.best_estimator_\n",
    "# Fit the random search object to the data\n",
    "rand_search_rf.fit(X_train, y_train)\n",
    "# Create a variable for the best model\n",
    "best_rf = rand_search_rf.best_estimator_\n",
    "# Fit the random search object to the data\n",
    "rand_search_cat.fit(X_train, y_train)\n",
    "# Create a variable for the best model\n",
    "best_cb = rand_search_cat.best_estimator_\n",
    "# Fit the random search object to the data\n",
    "rand_search_lgmb.fit(X_train, y_train)\n",
    "# Create a variable for the best model\n",
    "best_cb = rand_search_lgmb.best_estimator_\n",
    "# Print the best hyperparameters\n",
    "print(f\"Best hyperparameters for RF: {rand_search_rf.best_params_}.\")\n",
    "# Print the best hyperparameters\n",
    "print(f\"Best hyperparameters for XGB: {rand_search_xgb.best_params_}.\")\n",
    "# Print the best hyperparameters\n",
    "print(f\"Best hyperparameters for CAT: {rand_search_cat.best_params_}.\")\n",
    "# Print the best hyperparameters\n",
    "print(f\"Best hyperparameters for LGMB: {rand_search_lgmb.best_params_}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New_score: 0.8022988505747126\n",
      "New_score: 0.7839080459770115\n",
      "New_score: 0.7425287356321839\n",
      "New_score: 0.7954022988505747\n",
      "New_score: 0.7563218390804598\n",
      "New_score: 0.8\n",
      "New_score: 0.7770114942528735\n",
      "New_score: 0.8091954022988506\n",
      "New_score: 0.7793103448275862\n",
      "New_score: 0.8068965517241379\n",
      "New_score: 0.7977011494252874\n",
      "New_score: 0.8022988505747126\n",
      "New_score: 0.8137931034482758\n",
      "New_score: 0.8179723502304147\n",
      "New_score: 0.783410138248848\n",
      "New_score: 0.8456221198156681\n",
      "New_score: 0.815668202764977\n",
      "New_score: 0.8294930875576036\n",
      "New_score: 0.8064516129032258\n",
      "New_score: 0.7672811059907834\n"
     ]
    }
   ],
   "source": [
    "# KFold & Ensemble learning\n",
    "kfold = KFold(n_splits=20)\n",
    "cat_model = CatBoostClassifier(n_estimators=400, max_depth=8, learning_rate=0.4, verbose=False)\n",
    "xgb_model = xgb.XGBClassifier(max_depth=5, learning_rate=0.02, gamma=0.1, verbose=False)\n",
    "rf_model = RandomForestClassifier(n_estimators=700, max_depth=2, verbose=0)\n",
    "model_lgbm = LGBMClassifier(num_leaves=15, max_depth=12, learning_rate=0.05)\n",
    "\n",
    "estimators = [(\"catboost\", cat_model), (\"XGBoost\", xgb_model), (\"RandomForest\", rf_model), (\"LGBM\", model_lgbm)]\n",
    "best_score, best_model = 0, None\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    ensemble = VotingClassifier(estimators)\n",
    "    x_train, x_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    ensemble.fit(x_train,y_train)\n",
    "    new_score = ensemble.score(x_test, y_test)\n",
    "    print(f\"New_score: {new_score}\")\n",
    "    if new_score > best_score:\n",
    "        best_score = new_score\n",
    "        best_model = ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "D:\\Logiciel\\Progra\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "to_predict[\"Transported\"] = best_model.predict(to_predict.drop(columns=[\"Transported\"]))\n",
    "to_predict[\"Transported\"] = enc.inverse_transform(to_predict[\"Transported\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry: (4277, 1).\n",
      "Output: (4277, 16).\n",
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.15 / client 1.5.13)\n",
      "Successfully submitted to Spaceship Titanic\n"
     ]
    }
   ],
   "source": [
    "def generate_submission_file_based_on_prediction(result, source_file_path=\"data/sample_submission.csv\"):\n",
    "    df = pd.read_csv(\n",
    "        filepath_or_buffer=source_file_path,\n",
    "        sep=\",\",\n",
    "    ).set_index(\"PassengerId\")\n",
    "    print(f\"Entry: {df.shape}.\")\n",
    "    merge_df = pd.merge(\n",
    "        left=df,\n",
    "        right=result,\n",
    "        how=\"left\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "    print(f\"Output: {merge_df.shape}.\")\n",
    "    if df.shape[0] != merge_df.shape[0]:\n",
    "        raise ValueError(f\"Should be same size.\")\n",
    "    merge_df = merge_df[[\"Transported_x\", \"Transported_y\"]]\n",
    "    merge_df = merge_df.reset_index().rename(columns={\"Transported_y\": \"Transported\"}).drop(columns=[\"Transported_x\"])\n",
    "    merge_df.to_csv(\"data/my_submission.csv\", sep=\",\", index=False)\n",
    "    \n",
    "def generate_random_submission(source_file_path=\"data/sample_submission.csv\"):\n",
    "    df = pd.read_csv(\n",
    "        filepath_or_buffer=source_file_path,\n",
    "        sep=\",\",\n",
    "    )\n",
    "    df[\"Transported\"] = df[\"Transported\"].apply(lambda x: random.choice([\"False\", \"True\"]))\n",
    "    print(f\"File data/my_submission.csv successfully generated.\\n\")\n",
    "    df.to_csv(\"data/my_submission.csv\", sep=\",\", index=False)\n",
    "    \n",
    "def submit_submission(competition_id=\"2357\", submission_file=\"data/my_submission.csv\"):\n",
    "    with open(\"kaggle.json\") as credential:\n",
    "        json_credential = json.loads(credential.read())\n",
    "        os.environ[\"KAGGLE_USERNAME\"] = json_credential[\"username\"]\n",
    "        os.environ[\"KAGGLE_KEY\"] = json_credential[\"key\"]\n",
    "    result = subprocess.check_output(\n",
    "        [\n",
    "            \"kaggle\",\n",
    "            \"competitions\",\n",
    "            \"submit\",\n",
    "            \"spaceship-titanic\",\n",
    "            \"-f\",\n",
    "            submission_file,\n",
    "            \"-m\",\n",
    "            f\"{dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: New submission\",\n",
    "        ]\n",
    "    ).decode(\"utf-8\")\n",
    "    print(result)\n",
    "\n",
    "def get_latest_score(competition_id=\"2357\", team_id=\"10059555\"):\n",
    "    with open(\"kaggle.json\") as credential:\n",
    "        json_credential = json.loads(credential.read())\n",
    "        os.environ[\"KAGGLE_USERNAME\"] = json_credential[\"username\"]\n",
    "        os.environ[\"KAGGLE_KEY\"] = json_credential[\"key\"]\n",
    "        os.environ[\"KAGGLE_TEAM_ID\"] = team_id\n",
    "    result = subprocess.check_output([\"kaggle\", \"competitions\", \"submissions\", \"spaceship-titanic\"]).decode(\"utf-8\")\n",
    "    print(result)\n",
    "\n",
    "generate_submission_file_based_on_prediction(to_predict)\n",
    "submit_submission()\n",
    "# get_latest_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.15 / client 1.5.13)\r\n",
      "fileName               date                 description                          status    publicScore  privateScore  \r\n",
      "---------------------  -------------------  -----------------------------------  --------  -----------  ------------  \r\n",
      "my_submission.csv      2023-08-23 18:44:21  2023-08-23 19:44:18: New submission  complete  0.79798                    \r\n",
      "my_submission.csv      2023-03-24 11:21:24  2023-03-24 11:21:18: New submission  complete  0.79822                    \r\n",
      "my_submission.csv      2023-03-24 10:40:46  2023-03-24 10:40:42: New submission  complete  0.79120                    \r\n",
      "my_submission.csv      2023-03-24 10:36:54  2023-03-24 10:36:49: New submission  complete  0.79869                    \r\n",
      "my_submission.csv      2023-03-24 10:32:32  2023-03-24 10:32:18: New submission  complete  0.79448                    \r\n",
      "my_submission.csv      2023-03-17 15:44:11  2023-03-17 15:44:09: New submission  complete  0.79354                    \r\n",
      "my_submission.csv      2023-03-17 15:38:54  2023-03-17 15:38:49: New submission  complete  0.79845                    \r\n",
      "my_submission.csv      2023-03-17 11:44:11  2023-03-17 11:44:09: New submission  complete  0.79869                    \r\n",
      "my_submission.csv      2023-03-17 11:19:19  2023-03-17 11:19:12: New submission  complete  0.79845                    \r\n",
      "my_submission.csv      2023-03-17 09:53:09  2023-03-17 09:53:05: New submission  complete  0.79050                    \r\n",
      "my_submission.csv      2023-03-16 17:30:10  2023-03-16 17:30:06: New submission  complete  0.79354                    \r\n",
      "my_submission.csv      2023-03-16 17:02:46  2023-03-16 17:02:42: New submission  complete  0.79682                    \r\n",
      "my_submission.csv      2023-03-16 15:18:08  2023-03-16 15:18:05: New submission  complete  0.49964                    \r\n",
      "sample_submission.csv  2023-03-16 10:14:41  2023-03-16 10:14:38: New submission  complete  0.49310                    \r\n",
      "sample_submission.csv  2023-03-16 09:47:43  My first submission message          complete  0.49310                    \r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_latest_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
